{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Web Scraping with Python\n",
    "\n",
    "#### CEMFI Undergraduate Summer Internship 2021\n",
    "\n",
    "#### Instructor: Cay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Outline\n",
    "\n",
    "1. Introduction: Python and Jupyter Notebooks\n",
    "1. Web Scraping Example 1\n",
    "1. **Web Scraping: Example 2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal\n",
    "\n",
    "In this last hour of class we will deal with a case that requires a little bit more compared to our previous example.\n",
    "\n",
    "In particular, we will **focus** on how we can interact with the webpage (i.e. as if someone was using the browser)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium\n",
    "\n",
    "Selenium is a library that automates the interaction with web browsers. \n",
    "\n",
    "\n",
    "It can be used to validate web applications across different browsers and platforms. \n",
    "\n",
    "\n",
    "It can also be used to do web scraping to extract useful information contained in webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When will we need Selenium?\n",
    "\n",
    "With yesterday's approach (i.e. `requests`) we cannot interact with the page in a flexible way.\n",
    "\n",
    "\n",
    "We could only change the `url` and then request its content. \n",
    "\n",
    "\n",
    "With Selenium, we can accomplish different tasks, such as:\n",
    "- Scrolling\n",
    "- Clicking on buttons\n",
    "- Filling forms\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: jobs from Linkedin\n",
    "\n",
    "To illustrate the use of Selenium let's suppose that our goal is to scrape jobs from Linkedin.\n",
    "\n",
    "More specifically, suppose we are interested recent offers posted for the city of Madrid ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection\n",
    "\n",
    "We first go to the website to check how it looks like.\n",
    "\n",
    "One thing we want to pay attention to is the behavior of the **url**. \n",
    "\n",
    "Note that it changes when we choose a given filter, but not when we scroll down the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `requests` is not enough\n",
    "\n",
    "Let's do exactly what we did in the previous example to illustrate what we would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, in Python we do many imports\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send get request\n",
    "url = \"https://www.linkedin.com/jobs/jobs-in-madrid?keywords=&location=Madrid%2C%20Community%20of%20Madrid%2C%20Spain&locationId=&geoId=100994331&sortBy=R&f_TPR=r604800&f_PP=103374081&f_JT=F&f_E=4&position=1&pageNum=0\"\n",
    "result = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status code\n",
    "result.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's the standard way to import BeautifulSoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup parses the HTML content and python now understands the object \"soup\"\n",
    "source_code = BeautifulSoup(result.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first job listed\n",
    "job_postings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is the number of jobs so small?\n",
    "\n",
    "That's because the HTML code of this url only has information on the first jobs that appear in the search page.\n",
    "\n",
    "Let's go back to the website and see what's happening.\n",
    "\n",
    "Since the url does not change, we need to go beyond `requests`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "\n",
    "### Setup\n",
    "\n",
    "#### Installing Selenium \n",
    "\n",
    "We will need to install selenium, which can be done by following the same approach we took yesterday: `pip install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the webdriver\n",
    "\n",
    "To work with Selenium, another crucial ingredient is to have a **webdriver**. \n",
    "\n",
    "The **webdriver** is what will be automatically opening up your browser and interacting with it. \n",
    "\n",
    "The specific webdriver you need depends on which browser you prefer to use. \n",
    "\n",
    "Here I show how to do with *Google Chrome*, but you can download webdrivers of other browsers as well.\n",
    "\n",
    "For Google Chrome, you can download the webdriver at: https://chromedriver.chromium.org/downloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages: note that with selenium we don't use requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directoty where you have webdriver saved\n",
    "my_directory = %pwd + \"/chromedriver\"\n",
    "my_directory = my_directory + \"/chromedriver\"\n",
    "my_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access selenium driver\n",
    "driver = webdriver.Chrome(executable_path = my_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver loads the url we want\n",
    "\n",
    "# use the exact same url we used with requests\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many jobs we can scrape without scrolling down.\n",
    "\n",
    "Notice that we are not using `requests` but we still use BeautifulSoup to parse the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup gets the source code of the page the drive is at\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could have used **\"html.parser\"** (like in our previous code) instead of **\"lxml\"**, [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser) is a discussion of advantages and disadvantages of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, since we only loaded the page and called `BeautifulSoup` right away.\n",
    "\n",
    "#### Scrolling down \n",
    "\n",
    "We can use Selenium to scroll down the page so that it loads more jobs.\n",
    "\n",
    "First, we do it manually to show it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source code after manual scroll down\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, more jobs are loaded.\n",
    "\n",
    "We only need an automated way to scroll down.\n",
    "\n",
    "The cell below shows a general way to do for any website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general way to scroll down up to when there are no more jobs to load\n",
    "\n",
    "# time to wait for page to load correctly\n",
    "scroll_pause_time = 2\n",
    "\n",
    "# this command gets the height of the page \n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# while True is a loop that never stops (the condition is always satisfied) \n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(scroll_pause_time)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # test whether it is still possible to scroll down more or if it is the end\n",
    "    if new_height == last_height:\n",
    "        # If heights are the same it will exit the function\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code only stops because, after some time, Linkedin removes the scroll down option and a `See more jobs` button appears.\n",
    "\n",
    "Let's check whether we have more job listings that before in the source code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source code after manual scroll down\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we do have more jobs.\n",
    "\n",
    "How can we adjust the code to get even more jobs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clicking on buttons\n",
    "\n",
    "Notice that there is a `See more jobs` button at the end of the page.\n",
    "\n",
    "We can use Selenium to click on it so that more jobs appear.\n",
    "\n",
    "First, let's do it manually so that we make sure this action delivers the result we want (i.e. more jobs in the source HTML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source code after manual scroll down\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the number of jobs does increase.\n",
    "\n",
    "We can automate this action with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the ways to identify elements in the page (xpath)\n",
    "see_more_button = driver.find_element_by_xpath(\"//button[@aria-label='Load more results']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the object we just created \n",
    "see_more_button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can click on it as many times as we want, here as an example I click on it three times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example, I click on it three times\n",
    "for i in range(3):\n",
    "    see_more_button.click()\n",
    "    time.sleep(randint(1,2))\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(randint(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether we have even more jobs now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source code after manual scroll down\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we do. \n",
    "\n",
    "In general what we would want is\n",
    "- Scroll down until `See more jobs` button appears.\n",
    "- Click on `See more jobs` until it is no longer possible.\n",
    "\n",
    "Below is a way to click on a button as long as it exists (and is clickable).\n",
    "\n",
    "If the element is not found or not clickable the code stops and throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    # this explicitly waits for the element to be clikable\n",
    "    see_more_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, \"//button[@aria-label='Load more results']\")))\n",
    "    time.sleep(randint(1,2))    \n",
    "    driver.execute_script(\"arguments[0].click();\", see_more_button)\n",
    "    \n",
    "    # Scroll down to bottom just to facilitate visualization\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(randint(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the error`TimeoutException`.\n",
    "\n",
    "This tells that even after waiting for the time we allowed selenium to wait for, no element `\"//button[@aria-label='Load more results']\"` was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have all jobs listed that fulfil out initial search parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source code after manual scroll down\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a container with all jobs\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all jobs inside the container\n",
    "job_postings = results_list.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs?\n",
    "len(job_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last adjustment we can make is to tell our code to instead of throwing an error message, to simply exit the while loop when there is no `See more jobs` button in the page. \n",
    "\n",
    "This can be achieved with what is called *exceptions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: \n",
    "    \n",
    "    # on each iteration try to do this\n",
    "    try: \n",
    "        # this explicitly waits for the element to be clikable\n",
    "        see_more_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, \"//button[@aria-label='Load more results']\")))\n",
    "        time.sleep(randint(1,2))    \n",
    "        driver.execute_script(\"arguments[0].click();\", see_more_button)\n",
    "\n",
    "        # Scroll down to bottom just to facilitate visualization\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(randint(1,2))\n",
    "    \n",
    "    # if it encounters TimeoutException do not throw error, simply exit the while loop\n",
    "    except TimeoutException:\n",
    "        break\n",
    "\n",
    "# Scroll down to bottom\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(randint(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other commmon exceptions that are useful and appear frequently, I list them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more exceptions you use, the less likely is that the code will *brake*.\n",
    "\n",
    "On the other hand, it becomes more *black-box* and you may loose data with noticing.\n",
    "\n",
    "Thus, it is recommended to only include the exceptions that are indeed needed for each particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain information from each job\n",
    "\n",
    "The focus here is how to extract information, not what to do with it later on.\n",
    "\n",
    "Following this principle, the most natural piece of information of each job we should extract is its url.\n",
    "\n",
    "Once we have all job urls we can export a csv file to be used to later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of job url\n",
    "job_postings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each job has an id number associated to it.\n",
    "\n",
    "We don't need to save the entire url, since only the id is enough to enter the page describing the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].a.get('href').split(\"?ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].a.get('href').split(\"?ref\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].a.get('href').split(\"?ref\")[0].split(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].a.get('href').split(\"?ref\")[0].split(\"-\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "job_id  = job_postings[0].a.get('href').split(\"?ref\")[0].split(\"-\")[-1]\n",
    "job_url = \"https://es.linkedin.com/jobs/view\" + \"/\" + job_id\n",
    "job_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get job title and company\n",
    "\n",
    "We can also save job title and company in our csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].find(\"h3\",{\"class\":\"base-search-card__title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].find(\"h3\",{\"class\":\"base-search-card__title\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].find(\"h3\",{\"class\":\"base-search-card__title\"}).text.replace(\"\\n\",\"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].find(\"h4\",{\"class\":\"base-search-card__subtitle\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the url with href attribute\n",
    "job_postings[0].find(\"h4\",{\"class\":\"base-search-card__subtitle\"}).text.replace(\"\\n\",\"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it for all jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store info\n",
    "job_info_list = []\n",
    "\n",
    "\n",
    "for job in job_postings:\n",
    "    \n",
    "    # job title\n",
    "    title_tag = job.find(\"h3\",{\"class\":\"base-search-card__title\"})\n",
    "    title     = title_tag.text.replace(\"\\n\",\"\").strip() if title_tag else \"\"\n",
    "    \n",
    "    # company name \n",
    "    company_tag = job.find(\"h4\",{\"class\":\"base-search-card__subtitle\"})\n",
    "    company     = company_tag.text.replace(\"\\n\",\"\").strip() if company_tag else \"\"\n",
    "    \n",
    "    # job url \n",
    "    job_id_tag = job.a.get('href')\n",
    "    job_id = job_id_tag.split(\"?ref\")[0].split(\"-\")[-1] if job_id_tag else \"\"\n",
    "    job_url = \"https://es.linkedin.com/jobs/view\" + \"/\" + job_id if job_id_tag else \"\"\n",
    "     \n",
    "    # save info\n",
    "    job_info_list.append( [title, company, job_url]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at a few of them\n",
    "job_info_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### imports ####\n",
    "#################\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "###################\n",
    "##### control #####\n",
    "###################\n",
    "\n",
    "# time to execute\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#################\n",
    "##### paths #####\n",
    "#################\n",
    "\n",
    "# access selenium driver\n",
    "driver = webdriver.Chrome(executable_path = my_directory)\n",
    "\n",
    "\n",
    "######################\n",
    "##### source url #####\n",
    "######################\n",
    "\n",
    "# base url you want to scrape: try different cities, filters, etc.\n",
    "url = \"https://www.linkedin.com/jobs/jobs-in-madrid?keywords=&location=Madrid%2C%20Community%20of%20Madrid%2C%20Spain&locationId=&geoId=100994331&sortBy=R&f_TPR=r604800&f_PP=103374081&f_JT=F&f_E=4&position=1&pageNum=0\"\n",
    "\n",
    "# driver navigates to url and waits\n",
    "driver.get(url)\n",
    "time.sleep(randint(1,3))\n",
    "\n",
    "\n",
    "##############################\n",
    "##### interact with page #####\n",
    "##############################\n",
    "\n",
    "### 1. scroll down until see more button appears ###\n",
    "\n",
    "# general way to scroll down up to when there are no more jobs to load\n",
    "\n",
    "# time to wait for page to load correctly\n",
    "scroll_pause_time = 1.5\n",
    "\n",
    "# this command gets the height of the page \n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# while True is a loop that never stops (the condition is always satisfied) \n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(scroll_pause_time)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # test whether it is still possible to scroll down more or if it is the end\n",
    "    if new_height == last_height:\n",
    "        # If heights are the same it will exit the function\n",
    "        break\n",
    "    last_height = new_height\n",
    "    \n",
    "\n",
    "### 2. click see more button until all jobs are loaded  ###\n",
    "    \n",
    "while True: \n",
    "    \n",
    "    # on each iteration try to do this\n",
    "    try: \n",
    "        # this explicitly waits for the element to be clikable\n",
    "        time.sleep(randint(1,2))    \n",
    "        see_more_button = WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, \"//button[@aria-label='Load more results']\")))   \n",
    "        driver.execute_script(\"arguments[0].click();\", see_more_button)\n",
    "        \n",
    "        # Scroll down to bottom just to facilitate visualization\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(randint(1,2)) \n",
    "    \n",
    "    # if it encounters TimeoutException do not throw error, simply exit the while loop\n",
    "    except TimeoutException:\n",
    "        break\n",
    "\n",
    "# Scroll down to bottom (not necessary here, just to facilitate inspection)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "##############################\n",
    "######## get job info ########\n",
    "##############################\n",
    "\n",
    "# now we're back to what we know from BeautifulSoup, done with selenium\n",
    "\n",
    "# get source code after loading all jobs\n",
    "source_code = BeautifulSoup(driver.page_source, \"lxml\") \n",
    "\n",
    "# container with all jobs in page\n",
    "results_list = source_code.find(\"ul\", {\"class\":\"jobs-search__results-list\"})\n",
    "\n",
    "# create iterable object with all jobs in page\n",
    "job_postings = results_list.find_all(\"li\")\n",
    "\n",
    "# list to store info\n",
    "job_info_list = []\n",
    "\n",
    "# loop over all jobs found in result\n",
    "for job in job_postings:\n",
    "    \n",
    "    # job title\n",
    "    title_tag = job.find(\"h3\",{\"class\":\"base-search-card__title\"})\n",
    "    title     = title_tag.text.replace(\"\\n\",\"\").strip() if title_tag else \"\"\n",
    "    \n",
    "    # company name \n",
    "    company_tag = job.find(\"h4\",{\"class\":\"base-search-card__subtitle\"})\n",
    "    company     = company_tag.text.replace(\"\\n\",\"\").strip() if company_tag else \"\"\n",
    "    \n",
    "    # job url \n",
    "    job_id_tag = job.a.get('href')\n",
    "    job_id = job_id_tag.split(\"?ref\")[0].split(\"-\")[-1] if job_id_tag else \"\"\n",
    "    job_url = \"https://es.linkedin.com/jobs/view\" + \"/\" + job_id if job_id_tag else \"\"\n",
    "     \n",
    "    # save info\n",
    "    job_info_list.append( [title, company, job_url]  )\n",
    "    \n",
    "\n",
    "    \n",
    "######################\n",
    "##### export csv #####\n",
    "######################\n",
    "\n",
    "# 1. to dataframe\n",
    "# transform our list into a Pandas dataframe object\n",
    "df = pd.DataFrame(job_info_list, columns=['job_title', 'company_name', 'job_url'] )\n",
    "# 2. to csv\n",
    "df.to_csv('linkedin_jobs.csv', index=False)\n",
    "\n",
    "\n",
    "###################\n",
    "##### control #####\n",
    "###################\n",
    "\n",
    "time_elased = time.time() - start_time # now - minus when it started\n",
    "minutes = int( (time_elased)/60 )      # entire minutes\n",
    "seconds = time_elased % 60             # % is remainder operator \n",
    "print(\"--- %.0f minutes and %.0f seconds to scrape %.0f job listings  ---\" \n",
    "      % ( minutes, seconds, len(job_info_list) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] / df.company_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company_name.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.company_name==\"Amazon Web Services (AWS)\", \"company_name\"] = \"Amazon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company_name.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.company_name.value_counts(normalize=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_firms = 8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "firms = df.company_name.value_counts(normalize=True)[:n_firms].index\n",
    "\n",
    "job_offers = df.company_name.value_counts(normalize=True)[:n_firms]\n",
    "\n",
    "ax.bar(firms,job_offers)\n",
    "\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=13)\n",
    "ax.set_title(\"Fraction of jobs by company (top employers)\", size=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next\n",
    "\n",
    "Next, you could create another code to enter into each url scraped and get more specific information from each job in order to study characteristics of local labor markets:\n",
    "- Which industries are offering more jobs?\n",
    "- What are the most relevant skills to get a job and how does this vary by city/country?\n",
    "- Salaries.\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of course\n",
    "\n",
    "I hope you learned something useful for your future research projects.\n",
    "\n",
    "All the best to everyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
